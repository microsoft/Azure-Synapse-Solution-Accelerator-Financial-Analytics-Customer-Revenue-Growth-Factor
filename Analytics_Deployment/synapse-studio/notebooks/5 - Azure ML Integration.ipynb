{
  "metadata": {
    "saveOutput": true,
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copyright (c) Microsoft Corporation. \n",
        "Licensed under the MIT license. \n",
        "# Azure Machine Learning / AutoML Integration\n",
        "\n",
        "Capture results of AutoML experiments on the dataset and persist to Data Lake for reporting and analysis."
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Library Imports\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "outputs": [],
      "metadata": {},
      "source": [
        "import azureml.core\n",
        "from azureml.core import Workspace, Datastore, Dataset, Run\n",
        "from azureml.core.run import Run\n",
        "from azureml.core.experiment import Experiment\n",
        "from azureml.core.model import Model\n",
        "from azureml.interpret import ExplanationClient\n",
        "from pyspark.sql.functions import *\n",
        "import pprint"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Read Model Metrics from Azure ML\n",
        "\n",
        "Connect to the Azure ML workspace and extract metrics from the AutoML run."
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "outputs": [],
      "metadata": {},
      "source": [
        "# connect to Azure ML\n",
        "subscription_id = ''\n",
        "workspace_name = ''\n",
        "resource_group = ''\n",
        "\n",
        "ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
        "\n",
        "#pp = pprint.PrettyPrinter()\n",
        "#pp.pprint(ws.get_details())"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pull Metrics from Run\n",
        "\n",
        "Gather the following metrics:\n",
        "\n",
        "* AUC = AUC_weighted\n",
        "* Accuracy = accuracy\n",
        "* Precision = precision_score_weighted\n",
        "* Recall = recall_score_weighted\n",
        "* F1 = f1_score_weighted\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "outputs": [],
      "metadata": {},
      "source": [
        "# pull all metrics of best run\n",
        "experiment_name = ''\n",
        "run_id = ''\n",
        "\n",
        "experiment = Experiment(workspace=ws, name=experiment_name)\n",
        "fetched_run = Run(experiment, run_id)\n",
        "metrics = fetched_run.get_metrics()\n",
        "\n",
        "#pp = pprint.PrettyPrinter()\n",
        "#pp.pprint(metrics)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "outputs": [],
      "metadata": {
        "diagram": {
          "activateDiagramType": 1,
          "chartConfig": {
            "category": "bar",
            "keys": [
              "Metric"
            ],
            "values": [
              "Value"
            ],
            "yLabel": "Value",
            "xLabel": "Metric",
            "aggregation": "SUM",
            "aggByBackend": false
          },
          "aggData": "{\"Value\":{\"AUC\":0.7530583094702326,\"Accuracy\":0.6831256729733887,\"F1\":0.681859512812026,\"Precision\":0.682347667116298,\"Recall\":0.6831256729733887}}",
          "isSummary": false,
          "previewData": {
            "filter": null
          },
          "isSql": false
        }
      },
      "source": [
        "# select relevant metrics\n",
        "auc = metrics.get('AUC_weighted')\n",
        "accuracy = metrics.get('accuracy')\n",
        "precision = metrics.get('precision_score_weighted')\n",
        "recall = metrics.get('recall_score_weighted')\n",
        "f1 = metrics.get('f1_score_weighted')\n",
        "\n",
        "# combine into single dataframe\n",
        "metrics_df = sc.parallelize([['AUC', auc], ['Accuracy', accuracy], ['Precision', precision], ['Recall', recall], ['F1', f1]]).toDF(('Metric', 'Value'))\n",
        "\n",
        "#display(metrics_df)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Read Feature Importances from AutoML\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "outputs": [],
      "metadata": {},
      "source": [
        "client = ExplanationClient.from_run(fetched_run)\n",
        "engineered_explanations = client.download_model_explanation(raw=False)\n",
        "features_dict = engineered_explanations.get_feature_importance_dict()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "outputs": [],
      "metadata": {
        "diagram": {
          "activateDiagramType": 1,
          "chartConfig": {
            "category": "bar",
            "keys": [
              "Feature"
            ],
            "values": [
              "Value"
            ],
            "yLabel": "Value",
            "xLabel": "Feature",
            "aggregation": "SUM",
            "aggByBackend": false
          },
          "aggData": "{\"Value\":{\"avg_cart_abandon_rate_MeanImputer\":0.11945935832914051,\"avg_conversion_rate_per_user_per_month_MeanImputer\":0.20401384027776798,\"avg_order_value_per_user_per_month_MeanImputer\":0.41822444885119187,\"avg_session_duration_per_user_per_month_MeanImputer\":0.27078611121785473,\"brand_acer_purchased_binary_ModeCatImputer_LabelEncoder\":0.0175239679182456,\"brand_apple_purchased_binary_ModeCatImputer_LabelEncoder\":0.021093607022107382,\"brand_huawei_purchased_binary_ModeCatImputer_LabelEncoder\":0.0026359798356834024,\"brand_samsung_purchased_binary_ModeCatImputer_LabelEncoder\":0.008199318744508445,\"brand_xiaomi_purchased_binary_ModeCatImputer_LabelEncoder\":0.009319515351190353,\"product_id_1004767_purchased_binary_ModeCatImputer_LabelEncoder\":0.0006593092513365638,\"product_id_1004833_purchased_binary_ModeCatImputer_LabelEncoder\":0.0008278505122272443,\"product_id_1004856_purchased_binary_ModeCatImputer_LabelEncoder\":0.00024530348177285977,\"product_id_1005115_purchased_binary_ModeCatImputer_LabelEncoder\":0.006105109365293523,\"product_id_4804056_purchased_binary_ModeCatImputer_LabelEncoder\":0.0003915675165785674,\"sessions_per_user_per_month_MeanImputer\":0.40902442785462995,\"subcategory_audio_purchased_binary_ModeCatImputer_LabelEncoder\":0.0859806049597816,\"subcategory_clocks_purchased_binary_ModeCatImputer_LabelEncoder\":0.011233049590709289,\"subcategory_smartphone_purchased_binary_ModeCatImputer_LabelEncoder\":0.011650992130277638,\"subcategory_tablet_purchased_binary_ModeCatImputer_LabelEncoder\":0,\"subcategory_telephone_purchased_binary_ModeCatImputer_LabelEncoder\":0.00031129917547430985}}",
          "isSummary": false,
          "previewData": {
            "filter": null
          },
          "isSql": false
        }
      },
      "source": [
        "# save to list and convert numpy types to native\n",
        "features_list = []\n",
        "\n",
        "for key, value in features_dict.items():\n",
        "    temp = [key.item(),value.item()]\n",
        "    features_list.append(temp)\n",
        "\n",
        "# save to dataframe\n",
        "features_df = spark.createDataFrame(features_list, ['Feature', 'Value'])\n",
        "\n",
        "#display(features_df)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Results to Data Lake\n",
        "\n",
        "Persist the model results to CSV files on the Data Lake for reporting.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "data_lake_account_name = ''\n",
        "file_system_name = ''"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "metrics_df.coalesce(1).write.option('header', 'true').mode('overwrite').csv(f'abfss://{file_system_name}@{data_lake_account_name}.dfs.core.windows.net/reporting/model_metrics')\n",
        "features_df.coalesce(1).write.option('header', 'true').mode('overwrite').csv(f'abfss://{file_system_name}@{data_lake_account_name}.dfs.core.windows.net/reporting/feature_importances')"
      ],
      "attachments": {}
    }
  ]
}